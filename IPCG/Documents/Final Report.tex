\documentclass[a4paper,oneside,12pt,openany]{memoir}
\chapterstyle{section}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage[left=35mm, right=24mm, top=24mm, bottom=24mm]{geometry}
\usepackage{listings}
 \setlength{\beforechapskip}{1.5cm}
\setlength{\afterchapskip}{0.5cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.4cm}
% \setlength{\cftbeforechapterskip}{0pt}
\setcounter{tocdepth}{3}
\maxsecnumdepth{subsection}
\tightlists
\renewcommand{\bibname}{References}
\renewcommand*{\cftappendixname}{\appendixname~}
\renewcommand{\cftchapteraftersnum}{:}

\renewcommand*\descriptionlabel[1]{\hspace\labelsep \normalfont\bfseries --~~#1}

  \let\clearforchapter\relax  %to break chapter newpages.

\begin{document}
\frontmatter
\thispagestyle{empty}
\vspace*{\fill}
\begin{center}

{\Large \textsc{Electronics and Computer Science}} \\[0.3cm]
{\Large Faculty of Physical and Applied Sciences} \\[0.3cm]
{\Large University of Southampton} \\[2cm]

{\Large Thomas A. E. Smith} \\[0.2cm]
taes1g09@ecs.soton.ac.uk\\[0.3cm]
{\large \today} \\[1.5cm]

\textbf{\LARGE Intelligent Procedural Content\\[0.2cm] Generation for Computer Games} \\ [5cm]
% \end{center}
\begin{tabular}{r@{  --  }l}
{\Large Project supervisor: E. Gerding }&{\Large eg@ecs.soton.ac.uk} \\[0.3cm]
{\Large Second examiner: C. Cirstea}&{\Large cc2@ecs.soton.ac.uk} \\
\end{tabular}
\\[1.5cm]
% \begin{center}
{\large A project report submitted for the award of} \\[0.3cm]
{\large MEng Computer Science with Artificial Intelligence}

\end{center}
\vspace*{\fill}

\pagebreak
\vspace*{\fill}

\abstractintoc
\begin{abstract} %not more than 200 words
As the demand for ever larger and more varied computer game environments grows, procedural content generation (PCG) is increasingly used to ensure that content remains `fresh'. However, many of the opportunities to use these systems to generate truly personalised content have so far been largely overlooked. When content is generated manually or algorithmically during the design phase of a game, it can only be created according to the designers' expectations of the players' needs. By instead generating content during the execution of the game, and using information about the player(s) as one of the system's inputs, PCG systems should be able to produce more varied content that can be far more tailored to enhance individual players' experiences than anything manually created. In a related field, much has been written about the generation of player models from observed data, including for the purposes of adaptivity or dynamic difficulty adjustment (DDA), and literature exists examining the problem of generating satisfying game environments via challenge adjustment. This project looks at combining these fields to create a prototype `intelligent' PCG system (IPCG); specifically one that is capable of monitoring players' aptitude, and fully dynamically generating upcoming challenges to best suit their abilities. %[TODO: specify that this is a prototype, investigating a specific aspect of the many that IPCG is able to cover.]
\end{abstract}
\vspace*{\fill}
\vspace*{\fill}
\pagebreak

\tableofcontents
% \pagebreak

\chapter{Statement of Originality}
% - You are strongly encouraged to include a one or two paragraph statement of originality
% – “this is all my own work” is rarely true
% – you should acknowledge the help you have received
% - Was the idea for the project yours, or was it based on an earlier project, or your supervisor’s research?
% - The examiners will assume that the analysis, design, implementation, testing, \ldots are your own work
% - So tell them where this is not true
% – the design of component X follows a standard technique/pattern described in [source]
% – this is my own code except for [package/class/method] which I have copied from [Internet site/author]
% 
% Compton and Matheas for grammar
% one\_d SVG
% Own idea based on a particular personal interest in the field
% [TODO]
Aside from any part specified otherwise, this implementation and report is all my own work. It is based upon my own ideas born out of a particular interest in the field. Some elements of the design have been strongly influenced by existing work -- specifically the use of a probabilistic grammar for level generation, and the use of a one-dimensional support vector machine to calculate a decision boundary between two non-linearly-separable groups -- and this has been made clear in each respective section. I have also made extensive use of the standard Java packages.


\mainmatter

\chapter{Project Description}
\section{Introduction}
%1. Introduction describing the problem
%- A clear statement of the problem and goals of the project
%– project goals
Intelligent procedural content generation is a technique that may be used during the execution of a computer game. It involves using machine learning algorithms to intelligently distil information about the current state of the game into a form that may be used to affect ongoing procedural generation of content. The aim of this project is to investigate the use of intelligent procedural content generation (IPCG) in computer games, by looking at existing products, research in related areas and constructing a minimal prototype.

Due to the increasing demand for both detail and variety within computer game environments, various aspects of in-game content are now increasingly generated procedurally (i.e. algorithmically rather than manually), using techniques that are frequently simply refinements of algorithms used in the early days of computing, by games such as Elite \cite{elite} or Nethack \cite{nethack}.
However, one of the strengths of modern procedurally generated content (PCG) is that within reasonable limits it may be performed during runtime, allowing it to also use information about the player or current game state in order to dynamically generate content on-the-fly in response to the player's actions. 

In general, this will involve making use of algorithms from the field of Artificial Intelligence in order to evaluate the information available and condense it to a form suitable for input to a PCG system; hence such systems might reasonably be termed `intelligent' procedural content generators (IPCG), in contrast to merely `adaptive' procedural content generators which respond to specific parameters, or non-interactive PCG systems. Given the variety of mechanisms used by games to serve content to players, it can be difficult to define exactly what PCG is and is not. Togelius \textit{et al.} define PCG as ``\textit{the algorithmical creation of game content with limited or indirect user input}'' \cite{togeliusPCGdef}. This definition deliberately does not specify whether randomness is required in a PCG system, as examples of both random (stochastic) and deterministic PCG systems exist. It also does not distinguish between `offline' PCG, and adaptive `online' PCG, as both are used widely for varying purposes.

IPCG is an extension of typical PCG in that it will form a model of some aspect of the player or game state, and then use this as an input to an adaptive (parameterised) PCG system that modifies the generated output appropriately. Often this is done for the purposes of dynamic difficulty adjustment (DDA) -- in contrast to typical non-PCG DDA, which generally makes minor adjustments to existing content rather than generating new content -- though it can also be applied for a range of other purposes, from providing more of the type of content a player favours \cite{garmsr}, to directing the player towards unexplored areas \cite{radiant}. For the purposes of this project, a prototype will be developed which uses IPCG for DDA in basic 2D platformer levels.

In general, DDA is often viewed as the process of altering aspects of a game based on some part of the state of the game world - often a model of the performance of the player. Normally this is done with the intention of maintaining the player's ability to remain within a state of `flow' \cite{flow}. It can range from the simple `rubber-banding' used in basic racing games, to the more subtle alterations of timing, item placement and enemy frequency used by FPS games such as Resident Evil 5 \cite{ResE5}.

% [TODO use non- literature examples, improve flow here]


\section{Goals}
The key goals of this project are to investigate the current state of intelligent procedural content generation (IPCG) in computer games and academia, and to develop an example prototype that demonstrates each of the major components of an IPCG system. 
\begin{description}
\item[Investigate the use of IPCG in computer games:] To date, remarkably few commercial games have contained systems that could reasonably classified as IPCG. In order to successfully research the current use of IPCG then, it will be important to view the field in the context of both PCG and DDA, more mature techniques with more extensive bodies of existing literature and successful systems.
\begin{description}
\item[Existing commercial products:] A small number commercial products exist using IPCG systems, and many more are available that demonstrate use of PCG or DDA. Investigating these products will help to define the field of IPCG, and inform the development of the prototype.
\item[Existing academic literature:] Again, many academic papers have been written on the topics of PCG and DDA, and comparatively fewer about systems that could be described as IPCG. In general those that do cover the topic approach it as a more powerful form of DDA, overlooking the potential for other uses. 
\end{description}

\item[Construct a minimal prototype:]  In order to demonstrate the use of an IPCG system, it should be possible to develop a basic prototype that implements all of the required functionality of an IPCG system, and provides results that it would be difficult to otherwise obtain without using IPCG.
\begin{description}
\item[Specify requirements:] As with any software project, requirements will inform the development process and should serve to ensure the quality and achievability of the end product.
\item[Design and Implement:] Following a typical software development cycle, a system fulfilling the requirements of both the project and the wider IPCG definition should be developed.
\item[Test and Evaluate:] In order to evaluate the effectiveness of the system, one or more user studies will be necessary to investigate the general applicability of the solution, and highlight potential areas for further development.
\end{description}
\end{description}

\section{Prototype}
From investigation of existing literature, it can be seen that few demonstrations of successful IPCG exist. Following the efforts by Hunicke to establish the ``basic design requirements for effective dynamic difficulty adjustment" \cite{hunicke2005case}, this prototype will build upon those requirements and extend the concept into the field of IPCG. By combining ideas from existing literature on PCG and DDA systems, it should be possible to construct a limited system that performs all of the stages necessary for it to function as an example of IPCG. That is, it should monitor the game state, form a conceptual model of some aspect of the player, and then use that model in order to generate content tailored to the particular player in some way. Existing approaches, both in PCG and IPCG, have focused on simple 2D platforming systems, and so in order to benefit from developing the ideas presented in current literature, and also to aid in later comparative evaluation, it seems sensible to follow this approach also. For the purposes of this project, the prototype will be necessarily basic, performing only the functions sufficient to display working IPCG. It should also provide some facility for player-provided evaluation of the system, to aid in final analysis.
% [An analysis and specification of the solution to the problem.]

\section{Overview}
% [TODO: one paragraph]
This report will begin by investigating work in related fields, specifically PCG and DDA, as many of the techniques used for content generation and game state modelling are directly transferable to IPCG. Examples of both commercial systems and academic research will be used to illustrate certain features or ideas, and the concepts will be developed in a similar section on IPCG. This background will provide a starting point for specifying the requirements for the prototype system, which are necessary to define the scope of the desired system. These will then be developed into a detailed design, from which it should be possible to create the IPCG prototype. An account of the implementation will be followed by details of the system evaluation process and an analysis of the data collected. The report will close with an evaluation of the project as a whole, as well as the conclusions that can be drawn and suggestions for future work.

\chapter{Project Background}
% 2. a chapter reviewing approaches/literature
% - A review of the background literature

Though Intelligent Procedural Content Generation is the main focus of this project, in order to understand the approaches taken towards developing IPCG systems it is important to investigate the supporting work in the fields of PCG, and to a lesser extent DDA, which are closely related. An IPCG system cannot work without some mechanism in place to generate content appropriate for the model of the player that it has constructed, and the techniques and systems developed for the field of DDA are often relevant when attempting to construct this model - even if the purpose of the system is customisation of a game aspect completely unrelated to challenge. In contrast to the minimal body of academic literature concerning IPCG, both PCG and DDA are mature fields with plenty of research papers and example systems, and many commercial game products use these techniques. 

\section{Procedural Content Generation}
Procedural Content Generators in various forms have been used since the early days of gaming. Well-known games such as Elite and Rogue made extensive use of PCG, in order to present the player with expansive game worlds far larger than could be fully stored on the distribution media that was available at the time. In the case of Elite, this was done using a fully deterministic PCG system, and storing only the seeds used to generate the desired content - resulting in a game world that was identical each time it was generated, but that took very little memory to store. For Rogue, environments were generated pseudo-randomly, meaning a different play experience each time, but following strict constraints that ensured that levels were completable \cite{togeliusPCGdef}. 
As technologies improved, focus shifted more towards hand-crafted environments as it was easier to ensure that these provided value and did not feel sparse \cite{charbitat}. However, with the further progress of technology attention has returned to procedural generation. Modern game worlds typically contain vast amounts of detail, and procedural content generation algorithms are ideally suited to producing this by generating large numbers of variations on a theme -- be that trees, clouds, textures, or even sounds. Producing each of these items individually by hand would take many hours of labour and require large data files, but by defining specific sub elements and assembly rules, a small effort can lead to hundreds or thousands of variations. As PGC mechanisms have matured, they are once again being used for the provision of entire play environments. Commenting on the use of PCG in a successful commercial title (Borderlands, \cite{borderlands}), A. Doull claims that ``it points the way forward to a time where the current role of the level designer will be as obsolete as punch cards'' \cite{deathotleveldesigner}.



% [TODO: FIXME: mention occupancy regulated expansion / extension]

\subsection{Existing PCG Systems}
Many interesting PCG systems have been developed, too numerous to mention. Of particular note due to their comparative success:

\begin{description}
\item[Infinite Mario:] A Java reimplementation of the original Super Mario, Infinite Mario uses PCG to create an endless variety of potential levels \cite{mario}. Considered something of a standard in academic PCG implementations, the codebase is notable as it is often used as a launching point for AI competitions of various kinds - both pathfinding and alternative generation.
\item[Speedtree:] Possibly the most widely-used commercial PCG system in existence, Speedtree is a middleware application that generates trees and other vegetation for use in games and some movies \cite{speedtree}.
\item[Borderlands:] A commercially successful videogame, Borderlands was one of the first modern games to extensively use and publicise a non-decorative PCG system; in this instance to generate millions of different varied weapons \cite{borderlandsguns} - a selling point for the game. 
\end{description}

\section{Dynamic Difficulty Adjustment}
Another game design concept receiving increasing attention is dynamic difficulty adjustment (DDA). Typically, challenge adjustment within video games has consisted of user choice between one or more discrete challenge settings that have been painstakingly balanced at production time. However, this solution is far from ideal - typically, if a game is begun with a certain difficulty it is difficult to later change; and this upfront decision also alienates players that are unfamiliar with the terminology or expectations, or uncertain how to classify themselves \cite{lopes2011adaptivity}. Furthermore, since game difficulty is typically a continuous function of multiple parameters, it should be possible to precisely match each player to their ideal level of challenge rather than enforcing adherence to low-resolution skill profiles. 
By monitoring and then modelling the players' ability in some fashion, it can be possible to make informed changes to the play environment that satisfyingly help or hinder their progress. Typically, DDA is achieved by altering values that are hidden from the player, such as enemy health, accuracy, or the amount of ammo and health-kits available in the world \cite{hamlet}. Often, the intention is to do this invisibly, and merely ensure that the player remains optimally challenged. By manipulating values behind the scenes, it is possible to ensure that the player is neither over-challenged (leading to frustration / anxiety), or under-challenged (leading to boredom)\cite{flow}. As DDA systems are given more control over additional aspects of the game environment, they can begin to cross the line and enter the realm of PCG, fundamentally altering the structure and pacing of the player's experience. Much DDA literature is relevant to the field of IPCG, as the the data-collection and model-forming portions of IPCG systems have existing parallels in DDA research.

\subsection{Existing DDA systems}
From `rubber-banding' -- the simple modulation of maximum velocity by position used in racing games -- to the more complex level-based adjustments used in games such as Fallout, Oblivion and Homeworld, DDA systems are widely used in modern games. The best such systems often go unnoticed by the player \cite{moffett2010applying}.
\begin{description}
\item[Resident Evil 5:] A successful commercial game, Resident Evil 5 contained both a traditional difficulty selection screen and a far less publicised DDA system that used information about the player's health, equipment and difficulty setting in order to adjust the properties of enemies that they faced \cite{ResE5}.
\item[Hamlet:] Hamlet is a system designed by R. Hunicke to examine ``basic design requirements for effective dynamic difficulty adjustment" \cite{hunicke2005case}. It integrates with an SDK for an existing 3D game engine in order to monitor aspects of the player's status, and modifies upcoming encounters based on a historically-trained map between the result of player evaluation  and the desired game world adjustments.
\item[Cannon:] A system developed by Missura \textit{et al.} in order to investigate the use of K-means clustering and SVM clustering for effective DDA. Cannon is a very simplified 2D shooter that provides minimal gameplay but a wealth of monitoring and statistics \cite{missura2009player}.
\end{description}

\section{Intelligent Procedural Content Generation}
Although mechanisms fulfilling the definitions of IPCG systems have already started appearing in games, little has so far been written specifically on the subject - ``personalized and player-adaptive PCG [\ldots] is a new research direction'' \cite{shaker2010towards}. However, existing literature in related areas borders on the topic: in some cases DDA algorithms are being used to generate entire levels; thus qualifying as IPCG. One of the most thorough papers on this area is by Jennings-Teats \emph{et al.} \cite{jennings2010polymorph}, who developed Polymorph - a system that generates 2D platformer levels on-the-fly. Another academic IPCG system is Charbitat, which generates an entire 3D world dynamically based on the player's behaviour. Approaching the topic from another direction, Lopes' and Bidarra's survey of adaptivity challenges in games and simulations investigates the use of adaptivity in general in order to combat static and predictable content \cite{lopes2011adaptivity}, including via PCG.

\subsection{Existing Commercial IPCG Systems}
IPCG can be (and has been) used for a wide range of purposes, almost as varied as PCG itself. Three very different such uses in commercial games are detailed below. It is unsurprising that many of the existing applications of IPCG are used to tackle some of the current key challenges in game design: maintaining players' engagement with the game via enhancing immersion, and controlling the player's sense of `flow' \cite{flow}. %[TODO: link to earlier description of intentions for flow maintenance] 

\begin{description}
\item[Valve's `AI Director':] One of the most well-known such applications is used in Valve's games Left 4 Dead and Left 4 Dead 2. Known as the `AI Director', the system monitors the ``emotional intensity'' of each players' gameplay experience, by tracking factors such as each player's current health and recent kills, proximity and deadliness of visible enemies, and separation from the main group. It then dynamically alters the placement of supplies and the generation of enemies of various types in order to control pacing and maintain flow. It follows a policy of encouraging intensity to build up to a peak, sustaining threat for a short time, and then allowing intensity to fade during a `relax' period, thereby creating somewhat-unpredictable peaks and valleys during gameplay. In Left 4 Dead 2, the Director has additional control over the structure of the level \cite{valve}.
\item[Bethesda's Radiant Story:] Another recent example of IPCG is the Radiant Story system used in Bethesda's game Skyrim. Rather than monitor the player's performance and aptitude, it evaluates their progress and history. When the player receives a quest, the system looks for viable locations that the player has not yet explored, and customises details of the task so that it will take the player to this new location. This avoids requiring the player to return to already-completed areas, and instead forces exploration of previously unknown regions, helping to increase immersion and interest by avoiding repetition of content \cite{radiant}. 
\item[GAR's Weapon Evolution:] Finally, the weapon evolution mechanism in the game Galactic Arms Race\cite{garmsr} is an unconventional application of an IPCG system. All of the weapons in the game are represented by procedurally generated particle systems, with a small collection of variables that control their behaviour \cite{particles}. The IPCG system tracks only which weapons the player spends most time using, and then uses small neural networks to evolve new weapons that are variations on the player's favourite weapons so far. This allows the player to experience more of the type of content that they prefer, leading to increased engagement with the game.
\end{description}

\subsection{Existing IPCG Academic Literature}
In contrast to the idiosyncratic uses of IPCG in games, academic examples of IPCG are less obviously differentiable from the DDA and PCG systems that they have developed from. Papers these fields are in some cases converging towards describing IPCG, but only as extensions to existing techniques, rather than entirely new systems.

\begin{description}
\item[Polymorph: DDA Through Level Generation:]
%alters game during play, intensive more than just tweaking pervasive across level design/structure. Level generation and machine learning to understand component difficulty an player skill. 2D platformer with on-the-fly. Statistical model of difficulty in the level along with model of players current skill level. Machine learning from play traces from 'game-like tool'. This is an advance on prior work in dynamic difficulty adjustment, which has for the most part avoided adaptive level design, and in procedural level generation, which has mainly focused on creating full levels for replayability. Design for flow. Balancing takes effort. not possible 100\% coverage. DDA is typically numeric. this structural. Infinite mario not fly, this is. PCG typically more granular. rhythm. data collection tool. recognise shortcomings - and skew: critic modules. learn ranking - combination of component difficulty
Polymorph is a progression of prior work in both DDA and PCG: like previous DDA systems it alters challenge during play, and as with traditional PCG for 2D platformers it generates levels algorithmically. However, the DDA is effected via structural differences in the level design rather than numerical tweaking, and the level is generated online out of small rhythm segments \cite{smith2009rhythm}, rather than fully ahead of time. The authors present an interesting statistical model of difficulty, along with some of the issues and solutions found while evaluating the system \cite{jennings2010polymorph}.

\item[Charbitat:] 
Charbitat is a prototype game system that generates an entire play space procedurally, according to the play style of the gamer. Space generation is a deliberate feature of the player's interaction with the system, though in accordance with the definition of PCG \cite{togeliusPCGdef} it is performed only indirectly thorough analysis of the player's behaviour and in-game choices.
\item[Adaptivity Challenges in Games and Simulations: A Survey:]
% purposes: against static. worlds, scenarios and quests. low resolution stereotypes. individuality, different needs/desires. player-centred adaptivity. go beyond challenge as a steering purpose. training?. Purpose, then targets. traditionally AI
The survey is an investigation of present research into and existing commercial implementations of adaptivity in games. The topic has been broken down into three areas: purpose, target and method, with a wide range of examples given for each point raised. In contrast to other papers on the subject, the authors look beyond challenge as the sole steering purpose for adaptivity and discuss the wide range of types of content that may be adapted or generated algorithmically. Finally, they look at the methods by which content may be adapted or generated, and conclude that PCG is one of the most promising for offline generation, and also is increasingly suited to online adaptation \cite{lopes2011adaptivity}.
\end{description}

 
\chapter{Proposed System}
% 3. a chapter introducing final approach
% - An analysis and specification of the solution to the problem
% – detailed requirements

This section of the report provides a specification for the prototype IPCG demonstration system. Necessary components are discussed, and an approach suggested which is intended to result in a working system within the timeframe. The choices and assumptions made are justified, and a set of functional and non-functional requirements are listed in order to guide the development and evaluation of the system, as detailed in later chapters.

\section{Overview}
The project is intended to provide the minimum functionality necessary to demonstrate a working IPCG system. This means that it will require the ability to monitor game state as the player progresses, form a model of some aspect of the state, and then generate further content based on the input from the model.
Following the separation of concerns (SoC) design practice, the IPCG system may easily be modularised into three principle components: a `host' or base module, an adaptive PCG system, and an intelligent procedure for taking data about the game state and converting it into a model usable by the PCG. The proposed flow of information is shown in \fref{Figure:dataflow}.

\begin{figure}[htbp]
  \centering
  \includegraphics[trim = 10mm 12mm 10mm 12mm, clip]{system2}
  \caption{Data flow in proposed system.}
  \label{Figure:dataflow}
\end{figure}
\section{Modules}
Based on the divisions as defined above in \fref{Figure:dataflow}, each of the components may be developed as follows: %[FIXME]
\subsection{Game Base}
The game base need be nothing more than a simple 2D platformer engine. This module should handle all input and rendering activity, and should follow the Model-View-Controller architecture in order to facilitate monitoring and live updating. In addition to presenting the user with the output of the IPCG system, the Base module should provide the basic input handling, physics and game functionality necessary to play the platformer, while also logging any information about the participant's performance necessary for the evaluator module. 
\subsection{Adaptive PCG}
Forming the first portion of the IPCG system, this should be an adaptive (parameterised) 2D platforming level generator. Building on the work of Compton \emph{et al.} \cite{compton2006procedural}, this module should maintain a context-free grammar (CFG) of obstacles available, along with weights representing the estimated challenge of each element (terminal obstacle or combination). %[TODO: explain this more fully previously]
By taking these weights into account when deriving a string of obstacles from the CFG, sequences of a desired difficulty can be produced - or alternatively, the estimated challenge of existing sequences may be evaluated \cite{smith2011launchpad}. A PCG system designed in this way should be able to generate entire levels `offline', by maintaining pre-determined maxima and local variations in difficulty, but should also be capable of generating levels on-the-fly, by ensuring that short-term future difficulty levels match those requested by the Evaluator.
\subsection{Evaluator}
The second portion of the IPCG system should provide the capability to intelligently classify the player's performance in some way. Given the varied inputs from the Base module, the Evaluator should form a belief about the player's skill relative to the current challenge of the level. By running the player's data through a previously-trained classifier, this module should obtain a model that can be passed to the PCG module and then acted upon in order to generate further suitable content.

\section{Approach}
The modules above are presented in logical order of development: none of the IPCG system will be testable without the Base program (which can be tested standalone if given a hand-crafted level), but the PCG may be run and tested using specimen models, and finally the Evaluator can be tested once the other systems are in place. %[TODO: more here]
\subsection{Testing}
In order to ensure that the modules interact as expected, it will be important to define the interfaces between them and then use either test driven development (TDD) or unit testing in order to ensure that the modules conform to the specifications as expected. Integration testing will also be useful to check that the modules interact properly during execution of the final program.
\subsection{Classifier Training}
The development of the classifier will initially involve collecting data on as many potentially relevant features of the player's performance as possible, and then performing principle component analysis (PCA) upon the data-set in order to identify the maximally variant features. These can then be retained and used as the input to a K-means discretisation algorithm, which can finally be used to train a One-vs-All SVM classifier.
\subsection{Evaluation}
In order to perform retrospective evaluation of the system, some facility for users to provide deliberate feedback outside of the player modelling system will be necessary. Particularly after each period of interaction with the system, an opportunity to give feedback will provide relevant data, and it will be useful to also gather contextual information such as a participant's general level of familiarity with computer games.
\section{Justification}
The specification and approach as detailed should result in a working IPCG system, thereby fulfilling one of the goals of this project. Each aspect of the design has been chosen in order to produce the simplest possible demonstration prototype. For example, an alternative approach to the problem of generating a 2D platforming environment for use with DDA is presented by Sorenson \emph{et al.}\cite{sorenson2010towards}, who detail a more general top-down approach using genetic algorithms. However, their system is also more complex and provides an unneeded degree of generality for this project. This system as proposed should be able to fulfill the requirements given, and demonstrate intelligent variation in output based upon the skill of the player.
Initial research shows that much PCG, DDA and even some IPCG literature is applicable to the problem of 2D platforming. Existing approaches, both in PCG and IPCG, have focused on simple 2D platforming systems, and so in order to benefit from developing the ideas presented in current literature, and also to aid in later comparative evaluation, it seems sensible to follow this approach also. However, even more basic game environments exist - for example `Cannon', the very simple arcade style 2D shooter developed by Missura \textit{et al.} \cite{missura2009player} for the development of machine learning algorithms and player modeling for intelligent difficulty adjustment. Unfortunately, due to the natures of many of the simpler genres it becomes more difficult to modify the game environment in manners that are more obviously IPCG than DDA. 

\section{Requirements}
The main aim of the project requirements will be to constrain the problem to an achievable scale, and inform future evaluation of the final solution. The `functional' requirements that follow specify features that the system must provide. The majority of them are generic to any IPCG system, as this project intends to cover that area specifically, and refinements particular to this project are specified where necessary. The `non-functional' requirements define qualities that the system must adhere to, and in general are constraints that should serve to encourage feasibility and quality.

\subsection{Functional}
In order to properly implement IPCG, the system should:
\begin{description}
\item[Present the user with an interactive game environment.] Without at least a basic game environment in place, there will be no player interaction to collect data on, and nothing to generate content for. The system should provide a simple 2D platforming environment, with enough complexity to demonstrate working IPCG. Typical game mechanisms such as score or powerups are unnecessary in this context.
\item[Record data on the game state and player's behaviour.] The system will need to be able to monitor and record data on many aspects of the game environment. Statistics such as number of mistakes, number and average width of gaps jumped, and time to completion of level will all be needed for the player evaluator. In addition, non-player data such as the length of the level may also need to be taken into account.
\item[Evaluate this data according to specific criteria.] Using machine learning algorithms, it should be possible to process the monitoring input in some reliable manner, in order to retain the most useful aspects of the data while minimising surplus information. 
\item[Form a model of some aspect of the player.] The final form of the collected data should be a representative model that provides useful information about the player, and gives a clear indication of the necessary next actions by the IPCG system. 
\item[Use this model to inform further PCG activities.] Finally, the system will need to be able to make use of this model in order to generate future level chunks at a difficulty suitable for the player. To do this, the PCG must be able to evaluate the difficulty of its own output, and ensure that this matches the desired difficulty indicated by the evaluator.
\end{description}
In addition, in order to facilitate evaluation of the system itself, it should:
\begin{description}
\item[Keep a usable record of the data used to generate a level.] This serves two purposes: it should allow re-generation of a previous level when given the same input, for inspection of the generation process. It will also allow more accurate re-calibration of the classifier, should that be necessary.
\item[Request feedback from the player.] In order to evaluate the effectiveness of the system, it will be useful to obtain feedback from the actual users. Rather than require the use of an external platform, an inbuilt feedback facility could store responses alongside the in-game data stored above.
\item[Run on multiple operating systems and be widely available.] To ensure that the largest number of potential participants are able to use the system, it will need to be cross-platform and easy to access. Excessive size, acquisition difficulties or specific system requirements will all limit potential participation.
\item[Return information to a central storage point.] In order to recover information collected by the system, it will need to be capable of sending data back to a server once each session is finished. This can then be stored, collated and analysed in order to aid in system evaluation. 
\end{description}

\subsection{Non-Functional}
In order to remain at a manageable scale, the system should:
\begin{description}
\item[be written in Java.] The Java language has several properties which make it attractive for this project. It is supported on multiple platforms, and is capable both of launching from a website and then later communicating via http GET and POST  requests. It also has a large number of built-in libraries that support basic drawing and user interface widgets, reducing the burden of these portions of the implementation.
\item[be presented as a basic platformer.] Though IPCG can be applied in some fashion to most genres, adjusting jump and obstacle placements is likely to be one of the most basic applications that demonstrates convincing adaptability according to the output of the IPCG system.
\item[be confined to 2D.] As the degree of complexity of the generated content grows, so too does the complexity of the PCG required to generate it. A simple 3D terrain is relatively easy to generate, but in order to minimise the complexity of a platforming game it should be restricted to two dimensions. 
\item[limit the user to move and jump actions.] Many 2D platformers provide the player with novel interaction methods, which in this context would complicate the both the level generation and player evaluation systems. By restricting the player avatar's moveset, complexity is reduced.
\end{description}
In order to function satisfactorily as an interactive experience for the users, the system should also:
\begin{description}
  \item[consistently maintain the challenge of generated content.] Rather than generate content precisely matched to the participant's abilities, the system should provide challenges within a narrow range of difficulties centred on the target challenge level. This will ensure that the user will experience a variety of levels of challenge, including also continual opportunities to attempt content harder than that completed so far.  
  \item[conform to established gaming conventions.] In order to avoid confusing or alienating users that are already familiar with computer games of the same genre, the IPCG system should avoid breaking commonly understood conventions such as the goal existing at the far right of a level, or falling off the screen resulting in death.
  \item[remain responsive.] To avoid frustrating participants it is important that the system should not noticeably slow or become unresponsive during the execution of the program.
\end{description}
% [TODO: make this far longer and more detailed.  Write about requirements of/for evaluation method using swing]
 
 
% 4... further chapters discussing system implementation or experiment...
\chapter{Detailed System Design}
Following the problem specification and requirements given in the previous chapters, this section provides a more in-depth design for each of the components that will be necessary to the execution of the final system. After looking at the intended high-level structure and the development approach, each module is detailed with observations about its specific challenges and interactions with the rest of the system. 

\section{Overview}
In order to facilitate both presenting an IPCG system to the user and also recording feedback about that same system, the final program will need two main classes of interface: game segments that allow real-time interaction while the evaluator and generator are running, and a selection of feedback and instruction interfaces that enclose the interactive sections. In terms of high-level Java architecture and object types, the system should be written as a Java Applet using the Swing interface toolkit. This has the advantage that it will be able to be run online, making both distribution to participants and collection of feedback easier. The game segments can be drawn on Canvas objects using active rendering \cite{denault2007avoid}, and JPanels containing Swing widgets can be used for the feedback and instructive segments. The Java Swing toolkit will facilitate swapping between these two types of elements.

\section{Development Strategies}
Several basic software development approaches will be useful during the course of the project. Primarily, the class of software engineering process models known as `Agile' methodologies will help to keep the development process predictable and manageable. Agile methods follow an iterative cycle that promotes short sprints resulting in incrementally more functionality. They favour continual analysis of situation and workload, and help to ensure flexibility in planning. Another key component of the Agile philosophy is the use of a robust version control system and issue tracking, as alongside continuous integration and regression testing, these can ensure that a runnable version of the deliverable is always available, and the next incremental version is thoroughly specified. Though the techniques are typically used by small development teams, there exist modifications geared to solo developers \cite{nystrm2011agile}.

\section{Game Segments}
Interactive game segments form the link between the user and the IPCG system. While the user is playing, the system is continually monitoring the game state and generating new content as necessary. The components that make up the game segments each perform specific roles within the system, as detailed previously.
\subsection{Game Base}
The largest module within the system, the game base should be responsible for all user-facing aspects of the segment. Following the Model-View-Controller architecture, it can be further split into these separate components.
\begin{description}
\item[Model:] The model contains all of the data about the current game state, such as platform and player locations. In order to facilitate rendering, collision detection and challenge rating, this data can be stored within a scene-graph-like object hierarchy, as used in computer graphics. %\ref{Figure:model}.[FIXME] 
Individual platforms, gaps and obstructions are \textit{parts}, and an obstacle (\textit{component}) such as a jump or wall is a particular arrangement of a small group of parts. Components are built up into repeating \textit{patterns}, and multiple patterns are owned by a single \textit{level} object, which also owns the \textit{player} object and various useful metadata - this structure is based upon that used by \cite{compton2006procedural}. 
\item[View:] The view performs all rendering within the system. The view class should own the Canvas that is displayed on screen, and use double-buffering to smoothly render the correct portions of the model for the player. Ideally it should also be capable of rendering useful debug information for testing purposes.
\item[Controller:] The controller handles all interaction between the user and the rest of the system, and also all changes that occur to the model over time. it is also responsible for passing the game state to the evaluator when necessary, and updating the model with the output from the generator. Once the model has been altered appropriately, the view will render it to the screen for the player.
\end{description}

\subsection{Generator}
The generator module will rely heavily on the ability to calculate a specific difficulty rating for each obstacle based upon the properties of the physics system within the game. Using the kinematic equations of motion for particles with constant acceleration, each component will have a rating, and the \textit{level} and \textit{pattern} collections will each be able to produce a rating as a function of their children. When generating new content, the aPCG should be able to take a desired difficulty rating and produce a new component or pattern that is within some acceptable margin of the target rating. If the available components are ranked by difficulty then this can be done by selecting a random normally-distributed value, where the mean of the distribution is the desired rating, and the variance is some appropriate value based upon the acceptable margins. The closest valid component to this new value can then be added to the existing model.
% [FIXME add pseudocode, mention \cite{smith2011launchpad}, which uses a grammar based generator too.]

\subsection{Evaluator}
The evaluator module must continually monitor the game state, and use intelligent algorithms in order to process the information it receives into a form usable by the generator. The evaluator may perform regression or classification statically after being trained on a previously collected data set, or there are a number of incremental learning algorithms that would be capable of updating themselves with each new piece of data from the participant. A selection of metrics that the monitor could use:
\begin{description}
\item[The rating of each obstacle the player fails]
\item[The rating of each obstacle the player completes]
\item[The number of player successes]
\item[The number of player deaths]
\item[The overall velocity of the player]
\item[The time taken to complete the level]
\item[The length of the level]
\end{description}

If a sample data set is available, then one approach would be to discretise it into a number of groups using the K-means algorithm, and then train a multiclass SVM classifier on these groups. Classification during runtime would then consist of checking the current game state against the trained SVM.


\section{Feedback Interface}
In addition to the game segments presenting the IPCG system, participant feedback screens will be necessary in order to collect non-observational data concerning the participant's context and thoughts about the system. One of the advantages of using Java is that it provides the Swing GUI widget toolkit, which makes creating a simple interface allowing player feedback relatively simple. Once the information has been collected, it will need to be sent back to a central location for later analysis. 

\subsection{Panel Layouts}
Four distinct custom panel layouts will be needed: 
\begin{description}
\item[Plain text panel.] Used for the welcome screen with a brief overview of the experiment. Re-used for the interaction instructions, and then again after the final submission confirmation, for the thanks screen.
\item[Questionnaire panel.] Initial mini-questionnaire containing two labelled five-point Likert items to query the participants' familiarity with games and 2D platform games.
\item[Level evaluation panel.] Used after each of the four levels to request player feedback. A single seven-point Likert item with label and explanatory text, possibly level / progress statistics.
\item[Final submission panel.] The penultimate screen in the system. The submission panel should provide a non-editable textbox displaying all of the data to be submitted, and a final 'Submit' button for confirmation.
\end{description}

% [TODO: discuss justification for using likert items, advantages of symmetry. Cite ideal use of likert items in surveys, justifying neutral presentation.]

\subsection{Logging and Submission}
To collect data useful for evaluation and future calibration of the system, all of the player feedback and evaluator input and conclusions will need to be logged. A YAML-like simple parseable format should be sufficient to represent the types of data needed, in a human-readable format without excessive overheads. On the penultimate screen of the system, users will be given a chance to review the data that has been logged, before submission. On submission, the applet will attempt to POST the data to a php script hosted on a centralised server, which will save the data to a file. In case of error, the system may also attempt to save the data to a local file, and provide instructions on how to upload the file anonymously.
\subsection{Web Service}
In order to receive data from participants running the system remotely and anonymously, a simple .php script will be running at a hard-coded location on a centralised server. This will be able to receive POST data from the system, and after sanitising it save it to a file in a directory on the server. In case sending POST data from the Java Applet has failed, the script will also be linked to by a page that allows data file upload though a plain HTML form.
\subsection{Web Interface}
To allow potential participants to access the applet, it will need to be available online. However, in order to comply with Ethics requirements the system must not be accessible to users that have not viewed the participant information sheet and completed a consent form. The simplest way to enforce these requirements will be to present the system via a .php script that restricts access until consent has been given.
\subsection{Analysis}
In order to make use of the data collected, simple scripting can be used to parse the files into participant feedback data and participant interaction data. The feedback data can then be analysed using a mathematical tool such as Octave in order to evaluate the effectiveness of the system. Time permitting, the interaction data can be used to update the calibration data of the evaluator in order to improve its performance in preparation for a potential second survey. If necessary, it can also be inspected to determine the cause of any suspected anomalous feedback, or to give further insight into the veracity or cause of the results of analysis. 

\chapter{System Implementation}
% - The implementation

The design, requirements and context detailed by the previous sections provided a useful starting point and guide for the implementation of the system; however during the coding phase of the project certain inadequacies in the original design became clear. This section presents the components of the system in the same order as previously, and notes for each any significant choices made or problems encountered during the creation of that aspect. 

\section{Overview}
As the base visible component for the IPCG system (the game base's view's Canvas) already exists as a UI widget, placing it within the applet was done in the same manner as adding a JPanel or any other component. However, initialising the level required wrapping both the game segments and the feedback UIs in a minimal custom Screen interface. Switching between the two views also caused difficulties due to the way that Java handles contentPanes. 

\section{Development Strategies}
An Agile methodology was used throughout the project, with regular milestones and refactoring, and the aim of always having a working prototype at the end of each coding session. The overall system development intention was initially to follow a waterfall approach with regards to completing each module before moving onto the next; this proved to be fairly optimistic and na\"ive, as the modules were not fully independent. Defining common interfaces for the modules to share also proved difficult, as in many cases it was initially unclear how best to pass data between the classes, and so the boundaries remained fluid. While greater independence between the modules would be desirable, there were practical limits to the amount of time available, and so development of the three modules occurred largely simultaneously. Fortunately, the flexibility provided by the use of Agile methods meant that the project schedule was able to accommodate this fairly major alteration to the plan and adjust to the parallel development method instead.
%
\section{Game Segments}
As a result of the way the modules in the game segments interact, the final architecture can be seen as an MVCVC system. That is, in MVC terminology, the model is shared through the system, and then there are two views and two controllers. Within the game base, the view renders the level to the screen, through the GameObject's \texttt{render()} function. In response, the player provides input to the system, handled by the controller within the game base, which applies gravity and collision checking and -- when necessary -- calls upon the IPCG system to populate the model with more content. Within the IPCG system, the evaluator can be seen as a pure-data non-visual secondary view, which queries the model using the GameObject's \texttt{rate()} function, and calculates a classification for the current game state. When the generator is used, it accesses the classification and uses it as input to the aPCG, which can be seen as a secondary controller that modifies the model under certain conditions.
% [FIXME put a diagram here]

\subsection{Game Base}
The development of the game base module started along similar lines to previous Java projects: an applet containing a Canvas used for active rendering \cite{denault2007avoid}. Looking ahead to the need to link in with the PCG, the next step was to begin implementing the hierarchical model described in \cite{compton2006procedural} using a pseudo-component model, via interfaces such as \texttt{Drawable}, \texttt{Rateable} and \texttt{Collideable}, each containing a static collection that kept track of all of its members. However, when it came to implementing collision and debug rendering, the similarities between the hierarchical model used in the paper and the traditional scene graph pattern as used in computer graphics became apparent. This led to refactoring the internal data structures to follow a more traditional inheritance model. 

\subsubsection*{Model:}
Almost every object within the game inherits from the abstract \texttt{GameObject} base class, which provides the standard interface as shown in Listing~\ref{lst:gameobject}.

\lstset{language=Java, captionpos=b, basicstyle=\ttfamily\footnotesize,  breaklines=true}
\lstinputlisting[caption=GameObject.java, label=lst:gameobject]{../src/Model/GameObject.java}

This approach is augmented by the abstract child \texttt{GameCollection} class (Listing~\ref{lst:gamecollection}), which uses generics to support arbitrary collections of game objects, and provides default implementations for most of the required functions that simply redirect the same function to each of its children, in some cases culling by collision with the camera or player. 

\begin{lstlisting}[caption=Excerpt from GameCollection.java, label=lst:gamecollection]
public abstract class GameCollection<T extends GameObject> extends GameObject {

	ArrayList<T> elements;
\end{lstlisting}

Each of the concrete items within the world, such as \texttt{FlatPlatformPart}, inherit from \texttt{GameObject}, often via \texttt{Part}. Each of the collections inherit from \texttt{GameCollection}, specifying generics to restrict the type of object they can deal with, as in \texttt{Component.java}: \lstinline{public class Component extends GameCollection<Part>} .
% \subsubsection*{View}
\subsubsection*{Controller}
One unfortunate mistake made during the creation of the controller module was the early implementation of a physics engine too realistic to work well with the IPCG system. Writing the physics for a 2D platformer, even one that is deliberately as simple as for this project, turned out to be more of a challenge than originally anticipated. Initial research led to the `Sonic Physics Guide' by Mercury\footnote{\texttt{http://info.sonicretro.org/Sonic\_Physics\_Guide}}, which was broadly recommended as a good starting point. In terms of the game base alone, this resulted in solid, satisfying movement. However, the more complex nature of the physics had a significant impact on complexity of the evaluation of challenge for a particular jump or obstacle. As the project goal is to create a simple, minimal demonstration a lot of the more advanced features -- such as horizontal acceleration and variable jump responsiveness -- were removed, in order to simplify the challenge evaluation.


\subsection{Generator}
The generator module works largely as described in the design, with the exception that during the final stage rather than always picking the `nearest' obstacle to the target, it picks between the two nearest randomly in proportion to their distance. One issue encountered was the non-uniqueness of an obstacle's rating; this necessitated a change of data structure and partial rewrite of the algorithm in order to allow for multiple obstacles with the same rating. The method for placing the level end platform also proved difficult to fine-tune, and so in the final version it is automatically generated whenever the level exceeds a certain length.

\subsection{Evaluator}
Of all of the program components, the design for the evaluator changed most during the implementation phase. The starting intention to train an SVM on a large dataset classified via K-means was dependent upon the existence of a large dataset; however without a calibrated evaluator it would not be possible to run the study needed to acquire the data needed. The obvious conclusion would be to run a pre-study using a different machine learning algorithm that did not need large-scale training. Further research into this area indicated the feasibility of using a one-dimensional SVM to classify a player based upon the difficulty of the obstacles that they succeed or fail at \cite{su2002training}. By plotting the difficulty of failed obstacles against the rating of successfully completed ones, build two classes that are likely to be non-linearly-separable. However by using an SVM with slack variables, it is possible to calculate an optimal separating plane. This represents the perceived upper boundary of the participant's ability, and so by using this as the target input for the aPCG content can be generated that should challenge but not frustrate the player.

One major disadvantage of using a one-dimensional SVM compared to the original trained SVM design is that the 1D SVM must be retrained with each new piece of data, and this requires both processing power and space to store all of the previous training examples from the same session. In comparison, a trained SVM would only require the learned support vector values and a single classification check. Fortunately, Cauwenberghs \textit{et al.} provide an algorithm for incremental SVM leaning \cite{cauwenberghs2001incremental}, aspects of which were easily applicable to the one-dimensional SVM in use by the evaluator. This meant that far less data needed to be retained by the evaluator, and training was potentially much quicker.

% % \subsection{Interactions}
% % each module is detailed with observations about its specific challenges and interactions with the rest of the system. 
% % [TODO: detail callbacks, between components/systems.]
% % [Describe the entire quintuple model view controller system. I've already done this once somewhere, If I can find it. Don't know whether to put this in design, or as a realisation\ldots]
% 
\section{Feedback Interface}
% In addition to the game segments presenting the IPCG system, participant feedback screens will be necessary in order to collect non-observational data concerning the participant's context and thoughts about the system. One of the advantages of using Java is that it provides the Swing GUI widget toolkit, which makes creating a simple interface allowing player feedback relatively simple. Once the information has been collected, it will need to be sent back to a central location for later analysis. 
In contrast to the mathematically heavy implementations for the rendering, physics engine, generator and evaluator, the non-IPCG portions of the project were relatively simple. %[FIXME more here]
\subsection{Panel Layouts}
After experimenting with alternative layouts, it became apparent that the optimal design for the Swing portions of the program reinforced the goals of the interactive segments. Typically 2D platform games are designed so that the player character moves rightwards from the beginning of the level on the left, to the end of it at the far rightmost edge. In accordance with the non-functional requirement to avoid breaking videogame conventions, the IPCG segments of the program follow this tradition, and by mirroring this left-to-right flow throughout the user interface the Principle of Least Astonishment is preserved\footnote{http://en.wikipedia.org/wiki/Principle\_of\_least\_astonishment}.
% [FIXME more about likert]
% Four distinct custom panel layouts will be needed: 
% \begin{description}
% \item[Plain text panel.] Used for the welcome screen with a brief overview of the experiment. Re-used for the interaction instructions, and then again after the final submission confirmation, for the thanks screen.
% \item[Questionnaire panel.] Initial mini-questionnaire containing two labelled five-point Likert items to query the participants' familiarity with games and 2D platform games.
% \item[Level evaluation panel.] Used after each of the four levels to request player feedback. A single seven-point Likert item with label and explanatory text, possibly level / progress statistics.
% \item[Final submission panel.] The penultimate screen in the system. The submission panel shoud provide a non-editable textbox displaying all of the data to be submitted, and a final 'Submit' button for confirmation.
% \end{description}
% 
% % [TODO: discuss justification for using likert items, advantages of symmetry. Cite ideal use of likert items in surveys, justifying neutral presentation.]
% 
\subsection{Logging and Submission}
A subtle error in the logging code resulted in all of the obstacle components recording a difficulty of either 622 or 568 in the submitted data during the study, rather than any one of the 7 specific lower values that ought to have been logged. Fortunately the values were sufficiently spread that it was possible to reconstruct the original ratings with a high degree of certainty, and hence repair the affected data files.  
% To collect data useful for evaluation and future calibration of the system, all of the player feedback and evaluator input and conclusions will need to be logged. A YAML-like simple parseable format should be sufficient to represent the types of data needed, in a human-readable format without excessive overheads. On the penultimate screen of the system, users will be given a chance to review the data that has been logged, before submission. On submission, the applet will attempt to POST the data to a php script hosted on a centralised server, which will save the data to a file. In case of error, the system may also attempt to save the data to a local file, and provide instructions on how to upload the file anonymously.
\subsection{Web Service}
A flaw in the manual submission service led to the two data files that were uploaded using this method to be stripped of all line-breaks. However, the simple format of the files meant that it was possible to restore the files using a basic regex replace. 
% In order to receive data from participants running the system remotely and anonymously, a simple .php script will be running at a hard-coded location on a centralised server. This will be able to receive POST data from the system, and after sanitising it save it to a file in a directory on the server. In case sending POST data from the Java Applet has failed, the script will also be linked to by a page that allows data file upload though a plain HTML form.
\subsection{Web Interface}
The web interface caused the most problems out of all of the system components, due to the inconsistent support for embedding Java online across different web browsers. The final solution presented three different options for accessing the game after providing consent, to ensure that all potential participants were able to complete the study.
% To allow potential participants to access the applet, it will need to be available online. However, in order to comply with Ethics requirements the system must not be accessible to users that have not viewed the participant information sheet and completed a consent form. The simplest way to enforce these requirements will be to present the system via a .php script that restricts access until consent has been given.
% \subsection{Analysis}
% [FIXME]
% In order to make use of the data collected, simple scripting can be used to parse the files into participant feedback data and participant interaction data. The feedback data can then be analysed using a mathematical tool such as Octave in order to evaluate the effectiveness of the system. Time permitting, the interaction data can be used to update the calibration data of the evaluator in order to improve its performance in preparation for a potential second survey. If necessary, it can also be inspected to determine the cause of any suspected anomalous feedback, or to give further insight into the veracity or cause of the results of analysis. 













\section{Testing}
% Had significant problems throughout development when making things too general: specifically collision, among others. Initially worked on a general collision algorithm for colliding the player (a circle) with a platform of arbitrary angle. This turned out to be superfluous, as for simplicity the control physics does not deal differently with movement on non-horizontal surfaces, and the generator does not generate them to reduce complexity. This could be the subject of a future extension however. Confining all platforms to be horizontal greatly simplified the collision algorithms, though there are still difficult cases there the player is potentially colliding simultaneously with two contiguous platforms: in these cases the results of the two collisions are compared and whichever results in a greater y-displacement is accepted: due to the horizontality restriction on the platforms, this guarantees the player is no longer intersecting either possible collision (TODO no it doesn't, will have to look into this\ldots well, it does for contiguous platforms, but not for disjoint ones: $\_$- gah.)

Test-driven-development proved useful for several important subsystems; most notably the simplified collisions, the gap difficulty rating and the generator difficulty cache, as each of these held potential border case problems. Throughout development, regular integration testing was also necessary in order to ensure that new features and functions were compatible with existing code. 
%[TODO: Write about agile development and continuous unit and integration testing.][TODO: mention Java strong-typing and generics][TODO: write about]

\chapter{Project Evaluation}
% n-1. a chapter reviewing progress and results
% - A critical evaluation
% – measurements
There are two main ways to evaluate the success of this project: via the results of the user study, which indicate the effectiveness or otherwise of the IPCG system; and via critical comparison of the system and achievements to the initial requirements and goals.
% o Comparative evaluation(cf.competition)\\ 
% -– performance graphs, feature lists\\
% o Critical evaluation\\
% -– with respect to your project goals and plan\\
\section{User Study}
In order to help evaluate the effectiveness of the IPCG system, the final program was made available online for a period of days. During that time 27 participants completed the study and submitted data, providing enough information to come to useful conclusions about the success of the system.
\subsection{Study Design}
The study consisted of questionnaire sections alternating with single levels, either pre-generated or generated by the system during operation. 
The first questionnaire asked about the participant's familiarity with games in general, and 2D platform games specifically. After instructions on how to interact with the game, participants were presented with a simple pre-generated level with a specific target difficulty, and then asked to evaluate how challenged they were by the it. The process was then repeated with a more difficult pre-generated level. The system then generated a level suited to the observed ability of the player, and asked the participant to report how challenged they were. Another, more difficult level was generated, and a final evaluation performed. After a chance to review the collected data, the participants were thanked for taking part. A text copy of the interview questions is attached as Appendix B.

\subsection{Study Expectations}
If the IPCG system is effective, four statements can be made about the expected results from the user study, as follows:
\begin{description}
\item[1: Levels 1 \& 2:] `Familiar' players report less challenge than `unfamiliar' ones. \\Participants that self-report a greater familiarity with games should find the same level comparatively easier. This result says nothing about the system.
\item[2: Levels 1 \& 2:] Most players report that 2 was more difficult than 1. \\If level 2 was correctly generated with a greater difficulty than 1 as intended, then this result would indicate the effectiveness of the generator module. 
\item[3: Levels 3 \& 4:] Less variance in the reported difficulties than for 1 \& 2. \\If the evaluator is effective then levels 3 \& 4 should be closely tailored to each players' skill, leading to less variance in reported challenge.
\item[4: Levels 3 \& 4:] Most players should report that 4 was more difficult than 3. \\If items 2 and 3 hold, then the evaluator and generator are both effective. 
\end{description}

\subsection{Study Results}
Using the participants' data, and the property of Likert items that two items on the same scale may be summed to ease comparison, the following comments can be made about the original statements:
\begin{description}
\item[1: Levels 1 \& 2:] The Pearson's correlation between familiarity and challenge for the first two levels is close to 0. If greater familiarity led to lower reported challenge as expected then the coefficient ought to approach -1. However, this result is likely to be strongly affected by the fact that out of the 27 participants, only one reported a summed familiarity of less than 4, and in fact the average summed response was 6.8.
\item[2: Levels 1 \& 2:] The mean difficulty reported for level 2 was 25\% greater than the mean for level 1; %[FIXME and the paired t-test gave a value of 0.0033]; 
this is despite verbal feedback from several participants indicating that since they expected the second level of a game to be harder than the first, they would give the second level the same rating.
\item[3: Levels 3 \& 4:] The reported variance for levels 3 \& 4 actually increased by 15\%, indicating that the degree of fit worsened when the IPCG system was active. A number of possible causes for this result are discussed below.
\item[4: Levels 3 \& 4:] The mean difficulty reported for level 4 was 3\% less than the mean for level 3. 
\end{description}

\subsection{Study Conclusions}
Out of the four hypotheses that would indicate the effectiveness of the IPCG system, only one showed any positive evidence. Through observation of the results, three main conclusions have been reached:
\begin{description}
\item[1: The IPCG system is too aggressive.] In simplified terms, the system determines an optimal separation between obstacles that the player can manage, and obstacles that they cannot, and then repeatedly presents the player with obstacles from this region. However, in application, this causes problems as due to the nature of the algorithm, half of the time the player is presented with obstacles that the system believes are too hard -- occasionally, considerably so. This has lead to an IPCG version of the Peter Principle\footnote{http://en.wikipedia.org/wiki/Peter\_principle}; participants are raised precisely to their level of failure, rather than positioned a little below it.
\item[2: The pre-generated levels, and most of the available obstacles, are too easy.] The majority of the obstacles within the expressive range of the aPCG seem to be too easy for most players, leaving little granularity within what might be termed the `interesting' or useful range. Informal feedback included suggestions about altering non-obstacle aspects of the game -- such as player speed or checkpoint density -- in order to further increase the variety within the appropriate range of challenge.
\item[3: A wider range of participant ability is needed to properly test the system.] The vast majority of potential study participants would have been students of Computer Science or related fields, biasing the probability that they would self-report as `familiar' with computer games. This will have affected the results obtained from the study.
\end{description}
None of these problems are inherent to the IPCG system developed. If the study were to be run again after calibrating the system in light of these results, it is entirely possibly that supporting evidence could be obtained for all four original hypotheses.

\section{Critical Evaluation}
Despite the existence of calibration flaws discovered through the user study, in many other ways the system has succeed in fulfilling its requirements. The system correctly presents the user with and interactive game environment, records and evaluates game state data, and uses the resultant model to inform PCG activities. As demonstrated by the completion of the user study, it is also capable of recording playthrough and feedback data from multiple users on different systems, and returning this data for analysis. The system has also conformed to all of the non-functional requirements, though it could be argued that it pushes the challenge of generated content, rather than maintaining it as it ought.

However, despite technically fulfilling the requirements, it has not managed to demonstrate a successful IPCG system. Though it is entirely possible that the problem could be fixed through improved calibration, this does mean that one of the project's two main goals has not been fully attained. The investigation of IPCG in computer games is contained within this report, including a thorough review of academic literature relating to systems that could be described as IPCG, as well as related technologies that are important to the development of IPCG, such as PCG and DDA.
% Have constructed a system that provides something akin to a minimum working demonstration of IPCG.


\subsection{Reflection}
\begin{description}
\item[Tool use:] Tools used were generally sensible. Mercurial for version control and Eclipse as an IDE were both fairly invisible. One major advantage of Eclipse is that it also provides \LaTeX  authoring functionality through a plugin, allowing development of both system and report in a single familiar environment. Online diagram and flowchart webapp LucidChart\footnote{\texttt{http://www.lucidchart.com/}} proved invaluable for general design work.
\item[Techniques:] Constant refactoring was a definite boon. Not being afraid to rewrite entire sections made the code more flexible, and ensured the system was never committed to a particular way of doing things.
\item[Methods:] Using generics in the GameCollection class allowed very easy extension. Following the MVC architecture has simplified the separation of concerns, naturally leading to code that is more cohesive and less coupled. A certain degree of coupling has been unavoidable, as in the GameObject hierarchy, but it has transpired that the complete generality and independence called for by the schedule was both unnecessary and counterproductive.
\item[Goals and plan:] Goals were in the main sensible, though probably not well enough defined. `Investigate' is too vague: research was done with an eye specifically towards papers, examples and techniques that would aid in developing a demonstration system. This resulted initially in a very broad search, c.f Perlin noise generators. Thereafter the research direction honed in on PCG and DDA for 2D platformers, and was forced to remain more general for IPCG as there is comparatively little literature.
\end{description}

% Things I learnt: don't be afraid to throw out the current approach and rewrite it. Similarly, don't be afraid to ignore/rewrite the plan when it becomes apparent that it doesn't match the actuality of development. Don't reinvent the wheel! see engine mistake later. Don't plan for the ideal; more important to accept the practicalities of the project at hand.

% However (back on topic) there is scope for a more detailed review and comparison of any particulart technique in the literature than that provided here; particularly in investigating ways that various approaches could work together. In this instance, was forced to remain more abstract in order to also serve as an introduction to the field, and provide background for the prototype. 
% Plan was initially sensible, but suffered from becoming to broad and general throughout. More on this later.
% How could I have done it better differently?Brief list:
% Earlier investigation on the difficulties of sending POST data from Java. Have done a small amount of work with this in the past, but partly owing to time taken on other items, did not allow properly enough time for this.

% Given that the choice of a 2D platformer was probably a correct one, then rather than creating engine from scratch should have picked mature code. The IPCG system is quite demanding in that it must be tied into the engine at a level that allows it to monitor game state and provide upcoming content. At an early stage in the project, I investigated the availability of open-source engines, particularly in Java, but found none that were simple enough to not require a large time investment to learn the codebase, and that were structured in a way that would not prevent the IPCG system from working well. Had I found one, it would have saved time designing and implementing the game base module, albeit at the cost of having to integrate the design and implementation of the IPCG system with potentially complex existing code. However, none of the open-source engines that I found were usable, and as the game base was intended to be deliberately simple it appeared it would be quicker to write it myself [see the physics mistake]. However, I later discovered that by looking simply for an open source engine, I had missed Infinite Mario by Markus `Notch' Persson - an open source 2D platformer engine with inbuilt PCG system. If I had used that as an original code base, I would have been saved from many non-relevant project difficulties.
% [TODO discuss: Be more specific with requirements. Too much ambiguity leads to ambition and then disappointment.] 
% 
% Generality mistakes:
% Modules too general: designed and intended to operate fully independently. Unrealistic and unnecessary
% Initial research too general: many PCG techniques not directly applicable to 2D platformer
% Collision too general: arbitrary angles
% Location too general: ended up constricted to tiles
% Requirements unintentionally general: vague.





% Personal evaluation of system:
% Simpler than I would have desired, due to time constraints. Features that I would have liked to have included: \\
% investigation into whether (and how) placement of the player within the play surface affects difficulty, based on look-ahead time. i.e  $|\_o\_\_$ --| vs. $|\_  \_o\_\_|$\\
% angled platforms\\
% loopback cells (major effect on difficulty, don't die just return)\\
% death penalty: return to start of component, pattern or level\\
% auto-movement of camera\\
% Comparative evaluation with competition:\\
% difficult. closest competition is Polymorph, possibly Launchpad; can look at similarities and differences
% 
% Critical evaluation:\\
% has fulfilled all (most?) requirements. However, many desired features missing
% Important question: has it demonstrated working IPCG? If so, are there interesting comments that can be made `even with an environment this simple/restricted, it it possible to make meaningful changes that impact the player's experience of challenge'. If not, why not? was the system too simple, or is it purely bad calibration. In either case, is it possible to improve the calibration? 
% 
% \section{Metrics}
% 
% \chapter{Project Evaluation}
% % – comparative and critical evaluation – reflection
% One or more Gantt charts showing the planned schedule and the actual progress
% 
% o Critical evaluation\\
% – with respect to your project goals and plan\\
% o Reflection\\
% – in hindsight, did you use the right tools, techniques, metrics and methods?\\
% – what did you learn?\\
% – were your goals and plan sensible?\\
% – how could you have done it better/differently?\\

% Project Management and Planning
% o You should account for your time
% – this is the major expense for most projects
% o Compare your initial plan with how things actually went
% – perhaps include project diary as an appendix
% o If you fell behind how did you catch up – or decide which features to drop
% o Did you consider and allow for risks – illness, equipment failure or delays

% Things could have done differently: used the infinite mario codebase. Been more specific with requirements
% 
% Project Management and Planning\\
% o You should account for your time\\
% – this is the major expense for most projects\\
% o Compare your initial plan with how things actually went\\
% – perhaps include project diary as an appendix\\
% o If you fell behind how did you catch up – or decide which features to drop\\
% o Did you consider and allow for risks – illness, equipment failure or delays\\
% 
% Majority of the time spent in the initial reading phase on very broad subject background.\\
% First semester was more directed research, design work, interim report and what could loosely be called preparatory activities.\\
% Second semester moved onto implementation, to some extent according to the second gantt chart\\
% Did not allow time for: ethics submission. Player feedback implementation. Submission system implementation and difficulties. Naïvely separated modules. Timetabled testing as a separate activity: due to nature of agile methodology, testing was a continuous and integrated process.\\
% Definitely fell behind. Did not *exactly* end up dropping features: due to the unhelpful vagueness of the initial requirements, all of the functionality that was cut were investigatory topics that were not initially specified. The worst result from the lack of time was that the participant feedback is necessarily also fairly simple, reducing the number of interesting conclusions that can be drawn from it to simply whether or not the system works. If I had allowed more time for the wider testing phase, it would also have been interesting to perform a second study with a more accurately-calibrated system using the feedback from the first study, although possibly I will still be able to do this in time for the viva.
% At the time of writing, I thought that the plan was fairly generous. I was also fortunate in that I had no other academic coursework commitments during the second semester, and so believed that I would be able to stick to schedule uninterrupted. However, this proved difficult in the face of non-academic commitments, and was further hindered by the fact that the direction[] of actual development did not match the clean separation indicated in the original plan, leading to the use of ad-hoc milestones under the agile system. I believe that this flexibility was ultimately necessary, as making the individual modules general and robust enough to match the separate implementation indicated in the schedule would actually have taken far more time than the schedule allowed for. However, this same flexibility made it more difficult to reconcile the [actual] milestones with those indicated by the schedule, making it difficult to judge progress. 
% Scheduling aside, equipment failure would have been mitigated by the use of a cross-platform programming language, IDE and report-writing environment, and having all files (including bibtex file for references) backed up and under version control, allowing development to continue close-to-seamlessly on another machine. In retrospect, one weakness of this approach is the number of bookmarked resources and potential references that would have been lost if equipment failed.
% Scheduling allowed for a sensible mix of project work and other non-academic commitments, however expectations in this area proved unrealistic. During the research phase, ongoing reading took place alongside other commitments. During the implementation phase, it proved much more difficult to maintain continuous application to the project, partially due to the necessary wind-up time needed for effective coding, in comparison to the relative ease of quickly reading one or more potentially relevant papers. This resulted in the most effective coding taking place during irregular sprints - ideally suited to the agile methodology, but resulting in time lost to re-acquaintance overheads. 
% intensely educational experience

% Learned how different this report/project is from typically available academic literature. Almost all papers abstract away the development work (apart from key algorithms and techniques) and write instead only about results and conclusions. This leads to inaccurate beliefs about the areas of challenge in the project. 

\begin{figure}[ht]
\begin{minipage}[b]{0.5\linewidth}
\centering
\vspace{-5cm}
\section{Gantt Charts}

     \includegraphics[scale=0.7, trim = 12mm 30mm 95mm 30mm, clip]{done}
\caption{Semester 1 (actual)}
\label{fig:figure1}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.5\linewidth}
\centering
  \includegraphics[scale=0.5, angle=90, trim = 36mm 0mm 0mm 0mm, clip]{todo}
\caption{Semester 2 (planned)}
\label{fig:figure2}
\end{minipage}
\end{figure}


\chapter{Conclusions and Future Work}
% n. Conclusion and proposal of further work and ideas
% - Conclusions and future work
% – future work
% – summary and conclusions

% However: there are many interesting questions left unanswered. Even within the relatively simple 2D platforming environment, restricted to jumps, obstacles and movement, there are many gameplay variations left uninvestigated. The original requirements were not restrictive enough; c.f loopback, moving at varied speeds, varied lookahead, non-horizontal platforms, death penalties etc
% Though the plan turned out to be naïve in many respects, it did provide a useful starting point for guiding the overall structure of the implementation process. 

Overall, the project has met the majority of its stated goals, and so can be considered to some extent successful. However, in many ways more valuable have been the lessons learned from the mistakes made during the project. Certainly the most damaging mistake, made repeatedly, though in different contexts, was the desire for, specification of and implementation of overly general systems, without regards to the practicalities of the problem at hand 

One drawback of the system as proposed is that it must still reduce all of the data about the player's performance into a single discrete decision: whether to continue generating the level at the current difficulty, increase the difficulty, or decrease it. There is no opportunity for granularity representing player aptitude at a particular type of challenge. One possible extension would be to divide the obstacles by type (stationary hazard, timed hazard, projectile etc.), and evaluate the player's skill on particular classes individually, then use this more detailed model to inform a slightly more sophisticated PCG module. This approach would be an ideal candidate for a collaborative filtering algorithm, which with a large enough dataset would further allow the system to predict a player's aptitude at obstacle types that had not yet been seen by that player.

Further work on the existing system should definitely include efforts to provide more granularity at the more challenging end of the current expressive range. Further studies using the improved system will be necessary to ensure that the calibration is accurate.


\clearpage
\bibliography{IPCG}{}
\bibliographystyle{plain}

\appendix
\addcontentsline{toc}{chapter}{Appendices:}

\renewcommand{\printchaptername}{\chapnamefont\appendixname }
\renewcommand{\chapternamenum}{ }
\renewcommand{\printchapternum}{\chapnumfont \thechapter: }
%   \renewcommand{\afterchapternum}{: }


\chapter{Project Brief}

\begin{center}
{\LARGE Intelligent Procedural Content Generation\linebreak for Computer Games}


Thomas Smith \linebreak
\textbf{Supervisor:} Enrico Gerding
\end{center}

\textbf{Problem:} In modern computer game development, content production accounts for a large proportion of the initial (and in some cases, ongoing) outlay. As both budgets and in-game worlds get larger, there is increasing demand to offload some of these production efforts to automated systems. The concept of procedural content generators (PCG) has been around for some time, and they have been used in many successful games, but many of the advantages made available by these systems have so far been largely overlooked. When content is generated manually or algorithmically during the design phase of a game, it can only be created according to the designer's expectations of the players' needs. By instead generating content during the execution of the game, and using information about the player(s) as one of the system's inputs, PCG systems should be able to produce more dynamic experiences that can be far more tailored to enhance individual player's experiences than anything manually created. An intelligent procedural content generator (IPCG) should therefore consist of two parts: some means of evaluating (some aspect of) the player and generating a model, and a PCG system that is able to accept this model as an input and dynamically generate variants on its standard output based on the contents of the model.

%PCG
%There are several reasons for which we may wish to dynamically adjust the content provided by the IPCG systems, based on different aspects of the player. We may want to ensure that 

\textbf{Goals:} As specified above, an intelligent PCG should consist of two subsystems: an evaluator and its companion generator. The aim of the project will be to create a simple game-like application that uses an IPCG system to produce dynamically variable content based on the player's behaviour. I will begin by creating a variable PCG that is able to produce content based on specimen player models, and then use environments created in this way to create and tune a player evaluator for further generation.

\textbf{Scope:} In order to attempt to ensure that the project goals remain achievable, the scope should be restricted to the simplest possible system. Based on initial inspection of the problem space and existing literature, it appears that this would be adjustment due to player skill in a 2D platforming environment. The project will be coded in Java, as that comprises the majority of my recent coding experience, and it has a wealth of 2D graphics drawing support which will simplify the less-relevant areas of coding. Similarly, many of the peripheral components traditionally included in computer games are irrelevant to the project and will not be needed.
%Given that the majority of my recent coding experience is in Java, and the wealth of 2D graphics support in Java, 
% \end{document}


\chapter{Questionnaire Script}
\textit{These questions will be presented to participants during the course of the experiment:}
\\
\\

\textit{1) At the start of the experiment, rating agreement on a 5-point Likert item:}

\textbf{I am familiar with videogames in general.}

\begin{tabularx}{\linewidth}%
 {>{\raggedright}X%
  >{\centering}X%
  >{\raggedleft}X}
strongly~disagree & neutral & strongly agree\tabularnewline
\end{tabularx}
\begin{tabularx}{\linewidth}%
 {>{\centering}X%
  >{\centering}X%
  >{\centering}X%
  >{\centering}X%
  >{\centering}X}
$\circ$ & $\circ$ & $\circ$ & $\circ$ & $\circ$
\end{tabularx}
\\

\textbf{I am familiar with 2D platforming games.}

\begin{tabularx}{\linewidth}%
 {>{\raggedright}X%
  >{\centering}X%
  >{\raggedleft}X}
strongly~disagree & neutral & strongly agree\tabularnewline
\end{tabularx}
\begin{tabularx}{\linewidth}%
 {>{\centering}X%
  >{\centering}X%
  >{\centering}X%
  >{\centering}X%
  >{\centering}X}
$\circ$ & $\circ$ & $\circ$ & $\circ$ & $\circ$
\end{tabularx}
\\
\\
\\

\textit{2 –- 5) After each of the four interactive sections, evaluating the previous section on a 7-point Likert item:}

\textbf{The previous level presented a level of challenge that was:}

\begin{tabularx}{\linewidth}%
 {>{\centering}X%
  >{\centering}X%
  >{\centering}X%
  >{\centering}X%
  >{\centering}X%
  >{\centering}X%
  >{\centering}X}
   too easy & & & about~right & & & too hard\tabularnewline
$\circ$ & $\circ$ & $\circ$ & $\circ$ & $\circ$ & $\circ$ & $\circ$
\end{tabularx}
\\
\\
\\

\textit{6) At the end of the experiment:}

\textbf{This data has been collected during the course of this session:}

\begin{tabular}{|l|}
\hline
\texttt{< collected data >}\\
\\
\\
\hline
\end{tabular}


\fbox{Submit $>>$}

\chapter{CD Contents}
- A software project, or a project with a significant software component should include a CD or DVD at the back of the report with the sources and executables. A short table of contents for the CD/DVD should be included as a printed appendix.
TODO: once files are finalised. Possibly create a python script for this?
 
\end{document}

The main body of the report must not exceed 10,000 words. This is about 30 A4 pages of standard-spaced text, although display material such as diagrams and equations may add slightly to the page count.

The final report should provide:

A clear statement of the problem and goals of the project
A review of the background literature
An analysis and specification of the solution to the problem
A detailed design
The implementation
Testing strategy and results
A critical evaluation
Conclusions and future work
References to the literature
Appendices providing detailed technical references
The final report should document the project planning. In particular it should compare the project plans set out in the project brief and progress report with the actual achievements. To this end it should include the following:

A copy of the original project brief as an appendix
One or more Gantt charts showing the planned schedule and the actual progress
The report comprises a main body together with appendices. The appendices may contain a user manual, circuit diagrams etc. and support the main text. The nature and extent of the appendices will depend on the type of project undertaken and must be agreed in advance with the project supervisor. The text in the appendices is not included in the word limit.

Title page
Abstract
Contents list
Acknowledgments and Statement of Originality
1. Introduction describing the problem
- A clear statement of the problem and goals of the project
– project goals
2. a chapter reviewing approaches/literature
- A review of the background literature
3. a chapter introducing final approach
- An analysis and specification of the solution to the problem
– detailed requirements
4... further chapters discussing system implementation or experiment...
- A detailed design
– high level design or architecture
– comparison of alternative technologies
- The implementation
- Testing strategy and results
n-1. a chapter reviewing progress and results
- A critical evaluation
– measurements
– comparative and critical evaluation – reflection
n. Conclusion and proposal of further work and ideas
- Conclusions and future work
– future work
– summary and conclusions
References
- References to the literature
Appendices
- Appendices (if needed)
 - A software project, or a project with a significant software component should include a CD or DVD at the back of the report with the sources and executables. A short table of contents for the CD/DVD should be included as a printed appendix.
 



o A justified approach
– alternatives, feature list, selection, known-
good practice
o Claims supported by evidence
– measurements, audit trail, log book
o Appropriate use of tools, techniques, metrics and methods
– fit for purpose (engineering perspective) 
– to gain marks (educational perspective)


o Comparative evaluation(cf.competition) 
– performance graphs, feature lists
o Criticalevaluation
– with respect to your project goals and plan
o Reflection
– in hindsight, did you use the right tools, techniques, metrics and methods?
– what did you learn?
– were your goals and plan sensible?
– how could you have done it better/differently?

Project Management and Planning
o You should account for your time
– this is the major expense for most projects
o Compare your initial plan with how things actually went
– perhaps include project diary as an appendix
o If you fell behind how did you catch up – or decide which features to drop
o Did you consider and allow for risks – illness, equipment failure or delays


BCS Requirements
o the problem&the objectives of the project
o review of the context/literature/competition
o the life-cycle stages undertaken
o the development tools used
o use of V&V at each stage
o rationale for design/implementation decisions
o critical evaluation,review of the plan&any deviations from it, lessons learnt

Appropriate tools: Mercurial for source control, LucidChart for planning, Eclipse IDE

Need to write about testing. Collision system was TDD

o Interesting or important data structures
o Use pseudo-code or an activity diagram, but not flow charts, to explain interesting or important algorithms
– EL students may use ASM charts, however
o Use/reuse of class libraries or components
o Justified use of trendy technologies
– XML, web services, RSS, AI, encryption,\ldots
o Any interesting or significant errors – how you located and corrected them

Metrics
o Simple counts: LOCs, classes, methods, features implemented
o OO metrics: methods/class, coupling
o Test coverage: branches/methods executed
o Execution time (and complexity?)
o Memory usage (and complexity?)
o Achievement: tests passed, features delivered
o Satisfaction: end-user questionnaires
– appropriate descriptive/analytical statistics

o Which process did you follow: – waterfall, iterative, evolutionary?
o How does this show in your plan? 
o Why did you choose this process? 
o How well did it work out for you?

M Very good planning, progress, time management, and contingency planning
Detailed analysis of requirements, costs and benefits; very well designed and
T implemented; use of simulation/modelling/prototyping and a range of tools and
techniques
E Very good testing, critical and comparative evaluation and reflection
A Significant achievement and results; innovation and creativity; worthy of utilisation
Very well written, structured & formatted, review of related work including research papers
U Viva/demo and report shows good knowledge and understanding

\end{document}
